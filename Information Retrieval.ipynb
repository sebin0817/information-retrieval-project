{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63fffc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os, glob, re, sys, random, unicodedata, collections\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b1f4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_json(\"meta-data.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "acfd2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=a.loc[:,[\"ABSTRACT\",\"TITLE\",\"LANGUAGE\",\"DBRECORDID\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4071ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=a[(a[\"ABSTRACT\"].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f8041ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.index=range(0,len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dd6be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=pd.read_json(\"livivo_hq_100_candidates.jsonl\",lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5bb947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid:</th>\n",
       "      <th>qstr</th>\n",
       "      <th>candidates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>covid-19 OR sars-cov-2</td>\n",
       "      <td>[M33634079, M32328927, M32467201, M32333579, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>dementia</td>\n",
       "      <td>[M32016328, M32016329, M30925608, M31476229, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>biodiversity</td>\n",
       "      <td>[M31517113, M28569800, M32821644, M32295086, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>influenza</td>\n",
       "      <td>[M32048225, M31534258, M30722953, M31478058, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>malaria</td>\n",
       "      <td>[M30824502, M30550661, M29813596, M29813666, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1446</td>\n",
       "      <td>epigenetik</td>\n",
       "      <td>[M30643952, M29632998, M27299943, M29560499, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1452</td>\n",
       "      <td>stroke AND unit</td>\n",
       "      <td>[M33389780, M33394911, M33013663, M32115340, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1453</td>\n",
       "      <td>praxis AND der AND intensivmedizin</td>\n",
       "      <td>[M29556682, M22441683, M33053589, M32086542, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1454</td>\n",
       "      <td>das AND gesundheitssystem AND in AND deutschland</td>\n",
       "      <td>[M31594009, M31396676, M32023648, M31590198, M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1455</td>\n",
       "      <td>phenprocoumon</td>\n",
       "      <td>[M30809820, M30940121, M32649714, M32047440, M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     qid:                                              qstr  \\\n",
       "0       1                            covid-19 OR sars-cov-2   \n",
       "1       2                                          dementia   \n",
       "2       3                                      biodiversity   \n",
       "3       4                                         influenza   \n",
       "4       5                                           malaria   \n",
       "..    ...                                               ...   \n",
       "995  1446                                        epigenetik   \n",
       "996  1452                                   stroke AND unit   \n",
       "997  1453                praxis AND der AND intensivmedizin   \n",
       "998  1454  das AND gesundheitssystem AND in AND deutschland   \n",
       "999  1455                                     phenprocoumon   \n",
       "\n",
       "                                            candidates  \n",
       "0    [M33634079, M32328927, M32467201, M32333579, M...  \n",
       "1    [M32016328, M32016329, M30925608, M31476229, M...  \n",
       "2    [M31517113, M28569800, M32821644, M32295086, M...  \n",
       "3    [M32048225, M31534258, M30722953, M31478058, M...  \n",
       "4    [M30824502, M30550661, M29813596, M29813666, M...  \n",
       "..                                                 ...  \n",
       "995  [M30643952, M29632998, M27299943, M29560499, M...  \n",
       "996  [M33389780, M33394911, M33013663, M32115340, M...  \n",
       "997  [M29556682, M22441683, M33053589, M32086542, M...  \n",
       "998  [M31594009, M31396676, M32023648, M31590198, M...  \n",
       "999  [M30809820, M30940121, M32649714, M32047440, M...  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7e917",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "20345be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datas are stored as a list in every cell, this operation applied to extract the list\n",
    "for i in a.columns:\n",
    "    a[i]=a[i].apply(lambda x: x[0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9af5af20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eng               85375\n",
       "ger               35297\n",
       "lat                1212\n",
       "spa                 968\n",
       "por                 689\n",
       "                  ...  \n",
       "german                1\n",
       "Serbo-Croatian        1\n",
       "GER                   1\n",
       "English|German        1\n",
       "en-US                 1\n",
       "Name: LANGUAGE, Length: 72, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[\"LANGUAGE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e1b672ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_accents(text):   \n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    newText = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    return re.sub('[^a-zA-Z0-9 \\\\\\']', ' ', newText)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Make all necessary preprocessing of text: strip accents and punctuation, remove the words only contains digit\n",
    "    remove \\n, tokenize our text, convert to lower case, remove stop words and \n",
    "    words with less than 2 chars.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: cleaned tokenized text\n",
    "\n",
    "   \"\"\"    \n",
    "    WORD_MIN_LENGTH = 2\n",
    "    STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "    text = strip_accents(text)\n",
    "    text = re.sub(re.compile('\\n'),' ',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in STOP_WORDS and len(word) >= WORD_MIN_LENGTH]\n",
    "    words = [word for word in words if word.isdigit()==False]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bf1ccd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[\"ABSTRACT\"]=a[\"ABSTRACT\"].apply(lambda x: strip_accents(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "827f28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=(a[\"ABSTRACT\"].apply(lambda x: tokenize_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e3c205",
   "metadata": {},
   "source": [
    "## Building Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "91d5e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverted_index(words):\n",
    "    \"\"\"Create a inverted index of words (tokens or terms) from a list of terms\n",
    "\n",
    "    Parameters:\n",
    "    words (list of str): tokenized document text\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"       \n",
    "    inverted = {}\n",
    "    for index, word in enumerate(words):\n",
    "        locations = inverted.setdefault(word, [])\n",
    "        locations.append(index)\n",
    "    return inverted\n",
    "\n",
    "def inverted_index_add(inverted, doc_id, doc_index):\n",
    "    \"\"\"Insert document id into Inverted Index\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    doc_id (int): Id of document been added\n",
    "    doc_index (dict): Inverted Index of a specific document.\n",
    "\n",
    "    Returns:\n",
    "    Inverted index of document (dict)\n",
    "\n",
    "   \"\"\"        \n",
    "    for word in doc_index.keys():\n",
    "        locations = doc_index[word]\n",
    "        indices = inverted.setdefault(word, {})\n",
    "        indices[doc_id] = locations\n",
    "    return inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9377f95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ukolem': [0],\n",
       " 'teto': [1],\n",
       " 'prace': [2],\n",
       " 'bylo': [3],\n",
       " 'sestaveni': [4],\n",
       " 'jednoduche': [5],\n",
       " 'testove': [6],\n",
       " 'baterie': [7],\n",
       " 'ktera': [8],\n",
       " 'citlive': [9],\n",
       " 'registrovala': [10],\n",
       " 'zmeny': [11, 68],\n",
       " 'motoriky': [12],\n",
       " 'pareticke': [13],\n",
       " 'ruky': [14],\n",
       " 'postizene': [15],\n",
       " 'horni': [16],\n",
       " 'koncetiny': [17],\n",
       " 'pacientu': [18],\n",
       " 'po': [19, 50],\n",
       " 'cevni': [20],\n",
       " 'mozkove': [21],\n",
       " 'prihode': [22],\n",
       " 'chronickem': [23],\n",
       " 'stadiu': [24],\n",
       " 'vychazeli': [25],\n",
       " 'jsme': [26, 45],\n",
       " 'bezne': [27],\n",
       " 'dostupnych': [28],\n",
       " 'testu': [29, 56],\n",
       " 'nine': [30],\n",
       " 'hole': [31],\n",
       " 'peg': [32],\n",
       " 'test': [33, 118],\n",
       " 'dynamometrie': [34],\n",
       " 'apod': [35],\n",
       " 'ktere': [36],\n",
       " 'jsou': [37],\n",
       " 'pro': [38],\n",
       " 'tyto': [39, 67],\n",
       " 'ucely': [40],\n",
       " 'praxi': [41],\n",
       " 'nejvice': [42],\n",
       " 'vyuzivany': [43],\n",
       " 'sledovali': [44],\n",
       " 'charakter': [46],\n",
       " 'dynamiku': [47],\n",
       " 'jejich': [48],\n",
       " 'zmen': [49],\n",
       " 'rehabilitacnich': [51],\n",
       " 'procedurach': [52],\n",
       " 'vetsina': [53],\n",
       " 'vysledku': [54],\n",
       " 'vybranych': [55],\n",
       " 'mela': [57],\n",
       " 'obdobnou': [58],\n",
       " 'tendenci': [59],\n",
       " 'mirnemu': [60],\n",
       " 'zlepseni': [61],\n",
       " 'ale': [62],\n",
       " 'vzhledem': [63],\n",
       " 'rozsahu': [64],\n",
       " 'souboru': [65],\n",
       " 'nedosahly': [66],\n",
       " 'hladiny': [69],\n",
       " 'statisticke': [70],\n",
       " 'vyznamnosti': [71],\n",
       " 'prezentovane': [72],\n",
       " 'nalezy': [73],\n",
       " 'predkladame': [74],\n",
       " 'pouze': [75],\n",
       " 'jako': [76],\n",
       " 'vysledky': [77],\n",
       " 'pilotni': [78],\n",
       " 'studie': [79, 80],\n",
       " 'bude': [81, 84],\n",
       " 'pokracovat': [82],\n",
       " 'soubor': [83],\n",
       " 'dale': [85],\n",
       " 'rozsirovan': [86],\n",
       " 'aim': [87],\n",
       " 'paper': [88, 136],\n",
       " 'compile': [89],\n",
       " 'easy': [90],\n",
       " 'testing': [91],\n",
       " 'battery': [92],\n",
       " 'sensitive': [93],\n",
       " 'enough': [94],\n",
       " 'register': [95],\n",
       " 'changes': [96, 120],\n",
       " 'motor': [97],\n",
       " 'task': [98],\n",
       " 'performance': [99],\n",
       " 'paretic': [100],\n",
       " 'hand': [101],\n",
       " 'disabled': [102],\n",
       " 'upper': [103],\n",
       " 'limb': [104],\n",
       " 'patients': [105],\n",
       " 'brain': [106],\n",
       " 'stroke': [107],\n",
       " 'chronic': [108],\n",
       " 'stage': [109],\n",
       " 'selected': [110],\n",
       " 'tests': [111],\n",
       " 'commonly': [112],\n",
       " 'used': [113],\n",
       " 'rehabilitation': [114],\n",
       " 'scan': [115],\n",
       " 'characteristics': [116],\n",
       " 'dynamics': [117],\n",
       " 'value': [119],\n",
       " 'kinesiotherapy': [121],\n",
       " 'lessons': [122],\n",
       " 'outcome': [123, 134],\n",
       " 'trend': [124],\n",
       " 'slight': [125],\n",
       " 'improvement': [126],\n",
       " 'reach': [127],\n",
       " 'statistical': [128],\n",
       " 'signifi': [129],\n",
       " 'cance': [130],\n",
       " 'size': [131],\n",
       " 'group': [132],\n",
       " 'pilot': [133],\n",
       " 'presented': [135],\n",
       " 'going': [137],\n",
       " 'continue': [138],\n",
       " 'study': [139]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index(tokenized[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "627a66c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_doc_indexes = {}\n",
    "files_with_index = []\n",
    "files_with_tokens = {}\n",
    "for i in tokenized.index:\n",
    "    #Clean and Tokenize text of each document\n",
    "    words = tokenized[i]\n",
    "    #Store tokens\n",
    "    files_with_tokens[i] = words\n",
    "\n",
    "    doc_index = inverted_index(words)\n",
    "    inverted_index_add(inverted_doc_indexes, i, doc_index)\n",
    "    files_with_index.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5784a6ae",
   "metadata": {},
   "source": [
    "## Running Boolean Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3eaee288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boolean_search(inverted, file_names, query):\n",
    "    \"\"\"Run a boolean search with AND operator between terms over \n",
    "    the inverted index.\n",
    "\n",
    "    Parameters:\n",
    "    inverted (dict): Inverted Index\n",
    "    file_names (list): List with names of files (books)\n",
    "    query (txt): Query text\n",
    "\n",
    "    Returns:\n",
    "    Names of books that matchs the query.\n",
    "\n",
    "   \"\"\"      \n",
    "    # preprocess the user query using same function used to build Inverted Index\n",
    "    words = [word for _, word in enumerate(tokenize_text(query)) if word in inverted]\n",
    "    # list with a disctinct document match for each term from query\n",
    "    results = [set(inverted[word].keys()) for word in words]\n",
    "    # AND operator. Replace & for | to modify to OR behavior.\n",
    "    docs = reduce(lambda x, y: x & y, results) if results else []\n",
    "    return ([file_names[doc] for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "67d69323",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_with_index= pd.DataFrame(files_with_index)\n",
    "files_with_index.index=files_with_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0fd0e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[95362,\n",
       " 35075,\n",
       " 76420,\n",
       " 120962,\n",
       " 122115,\n",
       " 141,\n",
       " 107279,\n",
       " 5396,\n",
       " 81556,\n",
       " 93461,\n",
       " 58395,\n",
       " 22813,\n",
       " 107813,\n",
       " 50346,\n",
       " 49328,\n",
       " 38195,\n",
       " 47161,\n",
       " 124478,\n",
       " 107583,\n",
       " 91458,\n",
       " 97862,\n",
       " 109510,\n",
       " 58954,\n",
       " 22091,\n",
       " 7501,\n",
       " 99663,\n",
       " 43858,\n",
       " 121181,\n",
       " 34400,\n",
       " 27491,\n",
       " 6511,\n",
       " 9840,\n",
       " 44278,\n",
       " 16764]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean_search(inverted_doc_indexes, files_with_index[0] , c[\"qstr\"][15])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed4c8dd",
   "metadata": {},
   "source": [
    "## Ranking with tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "75f9a3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477456\n"
     ]
    }
   ],
   "source": [
    "DF = {}\n",
    "for word in inverted_doc_indexes.keys():\n",
    "    DF[word] = len ([doc for doc in inverted_doc_indexes[word]])\n",
    "\n",
    "total_vocab_size = len(DF)\n",
    "print(total_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5bdb4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = {} # Our data structure to store Tf-Idf weights\n",
    "\n",
    "N = len(files_with_tokens)\n",
    "\n",
    "for doc_id in tokenized.index:\n",
    "    tokens= tokenized[doc_id]\n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        # Calculate Tf\n",
    "        tf = counter[token] # Counter returns a tuple with each terms counts\n",
    "        tf = 1+np.log(tf)\n",
    "        \n",
    "        # Calculate Idf\n",
    "        if token in DF:\n",
    "            df = DF[token]\n",
    "        else:\n",
    "            df = 0\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        # Calculate Tf-idf        \n",
    "        tf_idf[doc_id, token] = tf*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4183fcb0",
   "metadata": {},
   "source": [
    "## Ranked Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "8227f37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_search(k, tf_idf_index, file_names, query):\n",
    "    \"\"\"Run ranked query search using tf-idf model.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): number of results to return\n",
    "    tf_idf_index (dict): Data Structure storing Tf-Idf weights to each \n",
    "                        pair of (term,doc_id) \n",
    "    file_names (list): List with names of files (books)\n",
    "    query (txt): Query text\n",
    "\n",
    "    Returns:\n",
    "    Top-k names of books that matchs the query.\n",
    "\n",
    "   \"\"\"   \n",
    "    tokens = tokenize_text(query)\n",
    "    query_weights = {}\n",
    "    for doc_id, token in tf_idf:\n",
    "        if token in tokens:\n",
    "            query_weights[doc_id] = query_weights.get(doc_id, 0) + tf_idf_index[doc_id, token]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for i in query_weights[:k]:\n",
    "        results.append(file_names[i[0]])\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6442cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_search(10,tf_idf,files_with_index[0], c[\"qstr\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1b744",
   "metadata": {},
   "outputs": [],
   "source": [
    "searched= ranked_search(100,tf_idf,files_with_index[0], c[\"qstr\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dabac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for i in searched:\n",
    "    result.append(a[a.index==i][\"DBRECORDID\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ce9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "((pd.DataFrame(result))==(pd.DataFrame(c[c[\"qid:\"]==1][\"candidates\"].values[0]))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfc5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a[\"DBRECORDID\"]==\"M33634079\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edebbf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[a.index==166241][\"ABSTRACT\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e0bfc",
   "metadata": {},
   "source": [
    "## Vectorization by query, eucledian normalization and ranking by cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc4c775",
   "metadata": {},
   "source": [
    "#### Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a4b5382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(q,tf_idf):\n",
    "    tokens = tokenize_text(q)\n",
    "    liste=[]\n",
    "    for i in list(tf_idf.keys()):\n",
    "        liste.append(i[0])\n",
    "    liste=set(liste)\n",
    "    liste = sorted(liste, key=lambda x: x, reverse=False)\n",
    "    query_weights_vector={}\n",
    "    for z in liste:\n",
    "        for token in tokens:\n",
    "            counter=0\n",
    "            if token in tokenized[z]:\n",
    "                counter += 1\n",
    "                if counter<=1:\n",
    "                    query_weights_vector[z]=[]\n",
    "                    for token in tokens:\n",
    "                        if token in tokenized[z]:\n",
    "                            query_weights_vector[z].append(tf_idf[z,token])\n",
    "                        else:\n",
    "                            query_weights_vector[z].append(0)\n",
    "    return query_weights_vector, tokens, (\"irish acute hospital service cure medicine query\")\n",
    "query_weights_vector, tokens, query = vectorize(\"irish acute hospital service cure medicine query\", tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703bee71",
   "metadata": {},
   "source": [
    "#### Eucledian Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8babf470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>irish</th>\n",
       "      <th>acute</th>\n",
       "      <th>hospital</th>\n",
       "      <th>service</th>\n",
       "      <th>cure</th>\n",
       "      <th>medicine</th>\n",
       "      <th>query</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.749977</td>\n",
       "      <td>0.661464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239687</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.626819</td>\n",
       "      <td>0.779165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239702</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.771608</td>\n",
       "      <td>0.636098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239782</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8901 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        irish     acute  hospital  service  cure  medicine  query\n",
       "3         0.0  0.000000  1.000000      0.0   0.0       0.0    0.0\n",
       "9         0.0  0.749977  0.661464      0.0   0.0       0.0    0.0\n",
       "48        0.0  0.000000  1.000000      0.0   0.0       0.0    0.0\n",
       "57        0.0  0.000000  1.000000      0.0   0.0       0.0    0.0\n",
       "80        0.0  0.000000  1.000000      0.0   0.0       0.0    0.0\n",
       "...       ...       ...       ...      ...   ...       ...    ...\n",
       "239687    0.0  0.626819  0.779165      0.0   0.0       0.0    0.0\n",
       "239702    0.0  0.000000  0.000000      1.0   0.0       0.0    0.0\n",
       "239777    0.0  0.771608  0.636098      0.0   0.0       0.0    0.0\n",
       "239782    0.0  0.000000  0.000000      1.0   0.0       0.0    0.0\n",
       "239785    0.0  0.000000  0.000000      0.0   0.0       1.0    0.0\n",
       "\n",
       "[8901 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = pd.DataFrame(query_weights_vector, index=tokens).T\n",
    "def normalizer(x):\n",
    "    y=x.apply(lambda x: x**2)\n",
    "    y=y.apply(lambda x: x.sum(),axis=1)\n",
    "    y=np.sqrt(y)\n",
    "    y=1/y\n",
    "    return normalize.apply(lambda x: x*y)\n",
    "normalized=normalizer(normalize);normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f914fc2",
   "metadata": {},
   "source": [
    "#### Cosine Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a147f1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_ranking(query_tokens, normalized, n):\n",
    "    v1 = np.array([1]*len(query_tokens))\n",
    "    output={}\n",
    "    for i in normalized.index:\n",
    "        v2 = np.array(normalized.loc[i,:].values)\n",
    "        output[i]= np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))\n",
    "    output = sorted(output.items(), key=lambda x: x[1], reverse=True)\n",
    "    liste=[]\n",
    "    for i in output:\n",
    "        liste.append(i[0])\n",
    "    return liste[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1673151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[126152, 65721, 87353, 140993, 58900, 133279, 144077, 41374, 94327, 140988]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_ranking(tokens, normalize, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd9995ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[166241, 65721, 184093, 197817, 39925, 126152, 56439, 144077, 13910, 860]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_search(10,tf_idf,files_with_index[0], query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4dd204",
   "metadata": {},
   "source": [
    "#### Let's see the queries with no match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5145d976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 / 1000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m c\u001b[38;5;241m.\u001b[39mindex:\n\u001b[1;32m----> 4\u001b[0m     rankings[(c\u001b[38;5;241m.\u001b[39mloc[i,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid:\u001b[39m\u001b[38;5;124m\"\u001b[39m])]\u001b[38;5;241m=\u001b[39m\u001b[43mranked_search\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtf_idf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfiles_with_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mqstr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(count,\u001b[38;5;28mlen\u001b[39m(c\u001b[38;5;241m.\u001b[39mloc[:,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqid:\u001b[39m\u001b[38;5;124m\"\u001b[39m])),end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36mranked_search\u001b[1;34m(k, tf_idf_index, file_names, query)\u001b[0m\n\u001b[0;32m     15\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize_text(query)\n\u001b[0;32m     16\u001b[0m query_weights \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc_id, token \u001b[38;5;129;01min\u001b[39;00m tf_idf:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     19\u001b[0m         query_weights[doc_id] \u001b[38;5;241m=\u001b[39m query_weights\u001b[38;5;241m.\u001b[39mget(doc_id, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m tf_idf_index[doc_id, token]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rankings={}\n",
    "count=0\n",
    "for i in c.index:\n",
    "    rankings[(c.loc[i,\"qid:\"])]=ranked_search(10,tf_idf,files_with_index[0],c.loc[:,\"qstr\"][i])\n",
    "    count+=1\n",
    "    print(\"{} / {}\".format(count,len(c.loc[:,\"qid:\"])),end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe88c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnksrtd = sorted(rankings.items(), key=lambda x: len(x[1]), reverse=False)\n",
    "def no_results(rnksrtd):\n",
    "    liste = []\n",
    "    for i in rnksrtd:\n",
    "        if len(i[1])==0:\n",
    "            liste.append(i[0])\n",
    "    return liste\n",
    "no_results(rnksrtd)\n",
    "c.loc[c['qid:'].isin(no_results(rnksrtd))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802c7648",
   "metadata": {},
   "source": [
    "##### %40 of the queries had 0 match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8440363",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_list=[]\n",
    "sayac=0\n",
    "for i in c[\"candidates\"]:\n",
    "    sayac+=1\n",
    "    print(\"{}/{}\".format(sayac, len(c)),end='\\r')\n",
    "    for z in i:\n",
    "        if z not in relevant_list:\n",
    "            relevant_list.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relevant_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70c347",
   "metadata": {},
   "source": [
    "#### Sum of relevant documents for the query list is 90668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f4d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sayac=0\n",
    "sayac2=0\n",
    "for i in a.DBRECORDID:\n",
    "    sayac2+=1\n",
    "    print(\"{}/{}\".format(sayac2, len(a)),end='\\r')\n",
    "    if i in relevant_list:\n",
    "        sayac+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a967cb",
   "metadata": {},
   "source": [
    "#### We only have 48041 document in the subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59f220b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sayac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d2566e",
   "metadata": {},
   "source": [
    "### Only 3227 out of 90668 exist in our subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5628add1",
   "metadata": {},
   "source": [
    "## BM25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c6746b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "13124e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = BM25Okapi(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ba08667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query= c[\"qstr\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4259f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores= bm25.get_scores(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "83f687f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_result(query,n):\n",
    "    tokenized_query = tokenize_text(query)\n",
    "    result_list = bm25.get_top_n(tokenized_query, tokenized, n=10)\n",
    "    keys=[]\n",
    "    for i in result_list:\n",
    "        key= [k for k,v in tokenized.items() if v==i]\n",
    "        keys.append(key[0])\n",
    "        keys=list(set(keys))\n",
    "    return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ba9e088a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[130531, 24196, 26566, 13872, 131765, 15702, 105978, 64155]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25_result(query,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "0d9e45a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5475,\n",
       " 18074,\n",
       " 128853,\n",
       " 85469,\n",
       " 96317,\n",
       " 14018,\n",
       " 20161,\n",
       " 17424,\n",
       " 11380,\n",
       " 28460,\n",
       " 121553,\n",
       " 81159,\n",
       " 94482,\n",
       " 117676,\n",
       " 77975,\n",
       " 116030,\n",
       " 64155,\n",
       " 41297,\n",
       " 27755,\n",
       " 64380]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_search(20,tf_idf,files_with_index[0], query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "090834fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ABSTRACT</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>DBRECORDID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>130531</th>\n",
       "      <td>Relevance  Laboratory diagnosis of the novel c...</td>\n",
       "      <td>Detectability of the novel coronavirus (SARS-C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COVID19::1e746e81c39fc21f50a1f9fe7ca1e39fe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24196</th>\n",
       "      <td>Retest Positive  for severe acute respiratory...</td>\n",
       "      <td>RetestÂ positive for SARS-CoV-2 RNA of \"recover...</td>\n",
       "      <td>eng</td>\n",
       "      <td>M32492212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26566</th>\n",
       "      <td>Purpose of review   Severe acute respiratory s...</td>\n",
       "      <td>Lethal zoonotic coronavirus infections of huma...</td>\n",
       "      <td>eng</td>\n",
       "      <td>M33660619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13872</th>\n",
       "      <td>Background  Laboratory diagnosis of the novel ...</td>\n",
       "      <td>Detectability of the novel coronavirus (SARS-C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COVID19::12632948535e57ddd33f6c84513eeef10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131765</th>\n",
       "      <td>Facing the ongoing pandemic caused by SARS CoV...</td>\n",
       "      <td>Unexpected diagnosis of COVID-19-associated di...</td>\n",
       "      <td>eng</td>\n",
       "      <td>M32890937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15702</th>\n",
       "      <td>Zoonotic coronavirus disease  COVID  has emerg...</td>\n",
       "      <td>The emerging SARS-CoV, MERS-CoV, and SARS-CoV-...</td>\n",
       "      <td>eng</td>\n",
       "      <td>M33642804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105978</th>\n",
       "      <td>Relevance  Laboratory diagnosis of the novel c...</td>\n",
       "      <td>Detectability of the novel coronavirus (SARS-C...</td>\n",
       "      <td>rus</td>\n",
       "      <td>BASE::ftarxivpreprints:oai:arXiv.org:2009.02962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64155</th>\n",
       "      <td>Coronavirus disease   2019  COVID 19  pandemic...</td>\n",
       "      <td>Molecular characterization and amino acid homo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>COVID19::110a169a630251f436b623abbec060c19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 ABSTRACT  \\\n",
       "130531  Relevance  Laboratory diagnosis of the novel c...   \n",
       "24196    Retest Positive  for severe acute respiratory...   \n",
       "26566   Purpose of review   Severe acute respiratory s...   \n",
       "13872   Background  Laboratory diagnosis of the novel ...   \n",
       "131765  Facing the ongoing pandemic caused by SARS CoV...   \n",
       "15702   Zoonotic coronavirus disease  COVID  has emerg...   \n",
       "105978  Relevance  Laboratory diagnosis of the novel c...   \n",
       "64155   Coronavirus disease   2019  COVID 19  pandemic...   \n",
       "\n",
       "                                                    TITLE LANGUAGE  \\\n",
       "130531  Detectability of the novel coronavirus (SARS-C...      NaN   \n",
       "24196   RetestÂ positive for SARS-CoV-2 RNA of \"recover...      eng   \n",
       "26566   Lethal zoonotic coronavirus infections of huma...      eng   \n",
       "13872   Detectability of the novel coronavirus (SARS-C...      NaN   \n",
       "131765  Unexpected diagnosis of COVID-19-associated di...      eng   \n",
       "15702   The emerging SARS-CoV, MERS-CoV, and SARS-CoV-...      eng   \n",
       "105978  Detectability of the novel coronavirus (SARS-C...      rus   \n",
       "64155   Molecular characterization and amino acid homo...      NaN   \n",
       "\n",
       "                                             DBRECORDID  \n",
       "130531       COVID19::1e746e81c39fc21f50a1f9fe7ca1e39fe  \n",
       "24196                                         M32492212  \n",
       "26566                                         M33660619  \n",
       "13872        COVID19::12632948535e57ddd33f6c84513eeef10  \n",
       "131765                                        M32890937  \n",
       "15702                                         M33642804  \n",
       "105978  BASE::ftarxivpreprints:oai:arXiv.org:2009.02962  \n",
       "64155        COVID19::110a169a630251f436b623abbec060c19  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.loc[keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "13894024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[130531, 24196, 26566, 13872, 131765, 15702, 105978, 64155]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668edf1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
