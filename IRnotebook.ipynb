{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fasttext import FastVector\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os, glob, re, sys, random, unicodedata, collections\n",
    "from tqdm import tqdm\n",
    "from functools import reduce\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.en.vec\n"
     ]
    }
   ],
   "source": [
    "eng_dictionary = FastVector(vector_file='wiki.en.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.de.vec\n"
     ]
    }
   ],
   "source": [
    "ger_dictionary = FastVector(vector_file='wiki.de.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading word vectors from wiki.fr.vec\n"
     ]
    }
   ],
   "source": [
    "fre_dictionary = FastVector(vector_file='wiki.fr.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n",
    "    \"\"\"\n",
    "    Source and target dictionaries are the FastVector objects of\n",
    "    source/target languages. bilingual_dictionary is a list of \n",
    "    translation pair tuples [(source_word, target_word), ...].\n",
    "    \"\"\"\n",
    "    source_matrix = []\n",
    "    target_matrix = []\n",
    "\n",
    "    for (source, target) in bilingual_dictionary:\n",
    "        if source in source_dictionary and target in target_dictionary:\n",
    "            source_matrix.append(source_dictionary[source])\n",
    "            target_matrix.append(target_dictionary[target])\n",
    "\n",
    "    # return training matrices\n",
    "    return np.array(source_matrix), np.array(target_matrix)\n",
    "\n",
    "def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n",
    "    \"\"\"\n",
    "    Source and target matrices are numpy arrays, shape\n",
    "    (dictionary_length, embedding_dimension). These contain paired\n",
    "    word vectors from the bilingual dictionary.\n",
    "    \"\"\"\n",
    "    # optionally normalize the training vectors\n",
    "    if normalize_vectors:\n",
    "        source_matrix = normalized(source_matrix)\n",
    "        target_matrix = normalized(target_matrix)\n",
    "\n",
    "    # perform the SVD\n",
    "    product = np.matmul(source_matrix.transpose(), target_matrix)\n",
    "    U, s, V = np.linalg.svd(product)\n",
    "\n",
    "    # return orthogonal transformation which aligns source language to the target\n",
    "    return np.matmul(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_words = set(eng_dictionary.word2id.keys())\n",
    "ger_words = set(ger_dictionary.word2id.keys())\n",
    "fre_words = set(fre_dictionary.word2id.keys())\n",
    "overlap = list(eng_words & ger_words)\n",
    "overlap_fr_en = list(eng_words & fre_words)\n",
    "bilingual_dictionary = [(entry, entry) for entry in overlap]\n",
    "bilingual_dictionary_fr_en = [(entry, entry) for entry in overlap_fr_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.021914206256188045\n",
      "0.04893054014859706\n",
      "-0.019676202184651607\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"cow\"]\n",
    "fre_vector = fre_dictionary[\"vache\"]\n",
    "ger_vector = ger_dictionary[\"kuh\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.73 GiB for an array with shape (772912, 300) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m source_matrix, target_matrix \u001b[39m=\u001b[39m make_training_matrices(ger_dictionary, eng_dictionary, bilingual_dictionary)\n\u001b[1;32m----> 2\u001b[0m transform \u001b[39m=\u001b[39m learn_transformation(source_matrix, target_matrix)\n",
      "Cell \u001b[1;32mIn [6], line 34\u001b[0m, in \u001b[0;36mlearn_transformation\u001b[1;34m(source_matrix, target_matrix, normalize_vectors)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m normalize_vectors:\n\u001b[0;32m     33\u001b[0m     source_matrix \u001b[39m=\u001b[39m normalized(source_matrix)\n\u001b[1;32m---> 34\u001b[0m     target_matrix \u001b[39m=\u001b[39m normalized(target_matrix)\n\u001b[0;32m     36\u001b[0m \u001b[39m# perform the SVD\u001b[39;00m\n\u001b[0;32m     37\u001b[0m product \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(source_matrix\u001b[39m.\u001b[39mtranspose(), target_matrix)\n",
      "Cell \u001b[1;32mIn [6], line 4\u001b[0m, in \u001b[0;36mnormalized\u001b[1;34m(a, axis, order)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnormalized\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, order\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[39m\"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     l2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mnorm(a, order, axis))\n\u001b[0;32m      5\u001b[0m     l2[l2\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m a \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(l2, axis)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\prana\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py:2560\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2557\u001b[0m     \u001b[39mreturn\u001b[39;00m add\u001b[39m.\u001b[39mreduce(\u001b[39mabs\u001b[39m(x), axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39mkeepdims)\n\u001b[0;32m   2558\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mord\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mord\u001b[39m \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m   2559\u001b[0m     \u001b[39m# special case for speedup\u001b[39;00m\n\u001b[1;32m-> 2560\u001b[0m     s \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39;49mconj() \u001b[39m*\u001b[39;49m x)\u001b[39m.\u001b[39mreal\n\u001b[0;32m   2561\u001b[0m     \u001b[39mreturn\u001b[39;00m sqrt(add\u001b[39m.\u001b[39mreduce(s, axis\u001b[39m=\u001b[39maxis, keepdims\u001b[39m=\u001b[39mkeepdims))\n\u001b[0;32m   2562\u001b[0m \u001b[39m# None of the str-type keywords for ord ('fro', 'nuc')\u001b[39;00m\n\u001b[0;32m   2563\u001b[0m \u001b[39m# are valid for vectors\u001b[39;00m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.73 GiB for an array with shape (772912, 300) and data type float64"
     ]
    }
   ],
   "source": [
    "source_matrix, target_matrix = make_training_matrices(ger_dictionary, eng_dictionary, bilingual_dictionary)\n",
    "transform = learn_transformation(source_matrix, target_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5374997100024734\n",
      "0.04893054014859706\n",
      "0.019419412966559853\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"cow\"]\n",
    "fre_vector = fre_dictionary[\"vache\"]\n",
    "ger_vector = ger_dictionary[\"kuh\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_matrix_fr, target_matrix_fr = make_training_matrices(fre_dictionary, eng_dictionary, bilingual_dictionary_fr_en)\n",
    "transform = learn_transformation(source_matrix_fr, target_matrix_fr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_dictionary.apply_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5374997100024734\n",
      "0.5894027260080404\n",
      "0.5092332499687042\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"cow\"]\n",
    "fre_vector = fre_dictionary[\"vache\"]\n",
    "ger_vector = ger_dictionary[\"kuh\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6232559990665913\n",
      "0.6797876352842949\n",
      "0.6651148554627233\n"
     ]
    }
   ],
   "source": [
    "eng_vector = eng_dictionary[\"city\"]\n",
    "fre_vector = fre_dictionary[\"ville\"]\n",
    "ger_vector = ger_dictionary[\"stadt\"]\n",
    "print(FastVector.cosine_similarity(ger_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, eng_vector))\n",
    "print(FastVector.cosine_similarity(fre_vector, ger_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "with pd.read_json('livivo_medline_00.jsonl', lines=True, chunksize=10000,nrows = 500000) as reader:\n",
    "    for chunk in reader:\n",
    "        df = df.append(chunk[['DBRECORDID','TITLE','ABSTRACT','LANGUAGE']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    df[i]=df[i].apply(lambda x: x[0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.query('LANGUAGE == [\"eng\",\"ger\",\"fre\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng    410066\n",
      "ger     21744\n",
      "fre     16052\n",
      "Name: LANGUAGE, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((df1.LANGUAGE).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vector_t = eng_dictionary[\"medication\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_vector_t = fre_dictionary[\"médicament\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_vector1 = ger_dictionary[\"sind\"]\n",
    "ger_vector2 = ger_dictionary[\"neue\"]\n",
    "ger_vector3 = ger_dictionary[\"medikamente\"]\n",
    "ger_vector4 = ger_dictionary[\"zu\"]\n",
    "ger_vector5 = ger_dictionary[\"teuer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_vector_t = ger_vector1+ger_vector2+ger_vector3+ger_vector4+ger_vector5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4781975866752953\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(eng_vector_t, ger_vector_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42318397682401143\n"
     ]
    }
   ],
   "source": [
    "print(FastVector.cosine_similarity(fre_vector_t, ger_vector_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"Make all necessary preprocessing of text: strip accents and punctuation, remove the words only contains digit\n",
    "    remove \\n, tokenize our text, convert to lower case, remove stop words and \n",
    "    words with less than 2 chars.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): Input text\n",
    "\n",
    "    Returns:\n",
    "    str: cleaned tokenized text\n",
    "\n",
    "   \"\"\"    \n",
    "    WORD_MIN_LENGTH = 2\n",
    "    STOP_WORDS = nltk.corpus.stopwords.words('english')\n",
    "    text = re.sub(re.compile('\\n'),' ',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in STOP_WORDS and len(word) >= WORD_MIN_LENGTH]\n",
    "    words = [word for word in words if word.isdigit()==False]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.query('DBRECORDID==[\"NLM100935395\", \"M28321727\", \"M22396211\", \"M26182251\", \"M27855450\", \"M27371084\", \"M17668771\", \"M15526640\", \"M12116571\", \"M17619416\", \"M15373101\", \"M17327990\", \"M17036952\", \"M12619228\", \"M17036953\", \"M10766551\", \"M15832754\", \"M12661441\", \"M11862798\", \"M12014270\", \"M9274293\", \"M15526639\", \"M15295687\", \"M14534866\", \"M14534865\", \"M11847881\", \"M17287946\", \"M11367987\", \"M16685628\", \"M15884497\", \"M6768988\", \"M11349619\", \"M4408140\", \"M3699648\", \"M10719459\", \"M5033568\", \"M4341201\", \"M5994901\", \"M5171652\", \"M7670011\", \"M572445\", \"M13674380\", \"M822301\", \"M13095574\", \"M31938804\", \"M5811151\", \"M530728\", \"M530727\", \"M530726\", \"M6015202\", \"M172696\", \"M13153044\", \"M4779158\", \"M5537878\", \"M7268345\", \"M5381164\", \"M13325698\", \"M530725\", \"M14128579\", \"M14271658\", \"M14943165\", \"M28382405\", \"NLM101607162\", \"M27903031\", \"M29623382\", \"NLM101237723\", \"M31828657\", \"M32865509\", \"M28711420\", \"M18726641\", \"M31379065\", \"M23212601\", \"M22160954\", \"M25612278\", \"M32291683\", \"M33055148\", \"M31191628\", \"M33518405\", \"M29335976\", \"M13137320\", \"NLM100888962\", \"M31227510\", \"M32983409\", \"M30874757\", \"M18447284\", \"M32737488\", \"M11565209\", \"M32165421\", \"M32350102\", \"M22454204\", \"M30928352\", \"M32543896\", \"M31133083\", \"M31211489\", \"M30253970\", \"M32511272\", \"M17290313\", \"M32127268\", \"M17334121\", \"M32964247\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_list = list(test_df[\"LANGUAGE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized=(df[\"TITLE\"].apply(lambda x: tokenize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_list_query = [fre_dictionary[item] for item in [\"cathéter\",\"cardiaque\"] if item in fre_dictionary]\n",
    "vecsum_query = np.sum(vec_list_query,axis=0)\n",
    "\n",
    "cosine_sim = []\n",
    "\n",
    "for token_docs in tokenized[0:1000]:\n",
    "    vec_list_doc = [ger_dictionary[item] for item in token_docs if item in ger_dictionary ]\n",
    "    vecsum_doc = np.sum(vec_list_doc,axis=0)\n",
    "    cosine_sim.append(FastVector.cosine_similarity(vecsum_query, vecsum_doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65872948, 0.63541166, 0.63005044, 0.6272868 , 0.62708371,\n",
       "       0.62147812, 0.61883629, 0.61870817, 0.61840581, 0.61742741,\n",
       "       0.61655217, 0.61428238, 0.61355099, 0.61244279, 0.61210313,\n",
       "       0.61053886, 0.60539011, 0.60487994, 0.60420434, 0.60242116,\n",
       "       0.60216802, 0.60201972, 0.60105402, 0.59974198, 0.5991122 ,\n",
       "       0.59792045, 0.5974014 , 0.59728187, 0.59698855, 0.59641539,\n",
       "       0.59561697, 0.59438333, 0.59345241, 0.59284673, 0.592762  ,\n",
       "       0.59230708, 0.59221347, 0.59207745, 0.59185406, 0.59180559,\n",
       "       0.59168474, 0.59105947, 0.59103073, 0.59046134, 0.58860881,\n",
       "       0.5879767 , 0.58554408, 0.58511775, 0.58501425, 0.58418249,\n",
       "       0.5840152 , 0.5830802 , 0.58305672, 0.58253369, 0.58242034,\n",
       "       0.58237253, 0.58221332, 0.58159155, 0.58149309, 0.58142414,\n",
       "       0.58122519, 0.58115302, 0.57968046, 0.57917339, 0.57815572,\n",
       "       0.57805633, 0.57755932, 0.57604149, 0.5758082 , 0.57576446,\n",
       "       0.57522375, 0.57477377, 0.57393907, 0.57283635, 0.57262601,\n",
       "       0.57153699, 0.57150067, 0.57141486, 0.57123467, 0.57110122,\n",
       "       0.57099071, 0.57080666, 0.57075782, 0.57052563, 0.57042796,\n",
       "       0.57040522, 0.56984455, 0.56978954, 0.56969211, 0.56959184,\n",
       "       0.56951328, 0.56930116, 0.5691587 , 0.56885996, 0.56879993,\n",
       "       0.56788343, 0.56786153, 0.5672657 , 0.56708471, 0.56661909])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(cosine_sim)[::-1][0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - implement 2 approaches \n",
    "\n",
    "1 translate query to english\n",
    "2 detect query language and use that"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35c754d8008c5f8560e4adf341ebf96f62d30db323e3ac43f60a1cb4dab6d757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
